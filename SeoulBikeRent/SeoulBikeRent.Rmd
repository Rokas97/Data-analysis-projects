---
title: "Seoul bike rent analysis (still in progress)"
author: "Rokas\n"
date: "Last compiled on `r format(Sys.time(), '%d %B, %Y')` "
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
<style>
body {
  color: #blue;
  font-family: Calibri;
  background-color: #lightblue;
}
pre {
  color: #708090;
  background-color: #F8F8FF;
}
</style>



# Dataset Attributes

Identifier

  -  Id - The Unique Integer Id for every entry in data set

Other variables (columns)

  -  Date - dd/mm/yyyy (String)
  -  Hour -  Hour of the day (int)
  -  Temperature - Temperature(°C) 
  -  Humidity -  Humidity (%)
  -  Wind_Speed - Wind Speed (m/s)
  -  Visibility - Visibility ( 10m range)
  -  Dew_Point - Dew Point Temperature(°C)
  -  Solar_Radiation - Solar Radiation (MJ/m^2)
  -  Rainfall - Rainfall (mm)
  -  Snowfall - Snowfall (mm)

Categorical Variables (columns)

  -  Season - Winter, Spring, Summer and Autumn
  -  IsHoliday - Yes/No stands for Holiday/No Holiday for Public Holidays from the South Korean Public Holidays List
  -  IsFunctioningDay - Yes/No stands for Yes/No to Whether the Hour in the entry is a functioning hour (work hour) of the day or not.

Target variable (column)

  -  Bikes_Rented - Number of Bikes Rented in the Hour of the Day with Specified parameters.
Dataset Attributes

Identifiers

    Id - The Unique Integer Id for every entry in both sets

Other columns

    Date - dd/mm/yyyy String for Date
    Hour - int, Hour of the day
    Temperature - Temperature in Degree Celsius
    Humidity - % of Humidity
    Wind_Speed - Wind Speed in m/s
    Visibility - Visibility in 10m range
    Dew_Point - Dew Point Temperature in Degree Celsius
    Solar_Radiation - Solar Radiation in MJ/m^2
    Rainfall - Rainfall in mm
    Snowfall - Snowfall in mm

Categorical Columns

    Season - Winter, Spring, Summer & Autumn
    IsHoliday - 1/0 for Holiday/No Holiday for Public Holidays from the South Korean Public Holidays List
    IsFunctioningDay - 1/0 for Yes/No to Whether the Hour in the entry is a functioning hour (work hour) of the day or not.

Target Column

    Bikes_Rented - Number of Bikes Rented in the Hour of the Day with Specified parameters.

## Libraries
```{r   results = 'hide', message = FALSE}
library(Hmisc)
library(corrplot)
library(plyr)
library(dplyr)
library(scales)
library(viridis)
library(lubridate)
library(knitr)
library(caTools)
library(leaps)
library(cvms)  #fro confusion matrix
library(earth) #for mars model
library(caret) # for mars tuning and cross-validation
library(vip)    # for variable importance plots(vips)
library(rpart)  #for decision trees
library(gam) #for GAMs
library(gbm) #for gbm
library(Metrics) #for gbm
library(xgboost)
library(rstatix)  #for outliers identification

```
## Exploratory analysis

Reading a file as a data.table and checking its structure and summary

```{r reading csv file}
seoul_bike <- read.csv("raw_seoul_bike_sharing.csv")
str(seoul_bike)
summary(seoul_bike)

```

Data frame consists of `14` variables or columns and `8760` observations or rows.
Let's look for `NA` values first and remove them by saving it in the new data frame.
There are `295` NA values of `RENTED_BIKE_COlUMN`, and `11` `NA` values of `TEMPERATURE`.
We can remove them


```{r remvoing NA values}

sapply(seoul_bike, function(x) sum(is.na(x)))
clean_bike <- na.omit(seoul_bike)
str(clean_bike)
```

 All the names of `14` variables are converted to lower cases for convenience.
 
 
```{r lower cases}
names(clean_bike) <- tolower(names(clean_bike))
names(clean_bike)
```



By using a `lubridate` package date is converted from char format to date format with `dmy` function.(day/month/year).
Also categorical variable `holiday' is changed to numerical values for calculations and `seasons' names are changed to factors. New variable `day` added to `clean_bike` data frame
to name weekdays for analysis. Weekdays are converted to factors as well.


```{r cleaning}
clean_bike$date <- dmy(clean_bike$date)
clean_bike$holiday <- ifelse(clean_bike$holiday == 'No Holiday' ,0, 1)
#clean_bike$seasons <- recode(clean_bike$seasons, 'Winter'= 1, 'Spring'= 2, 'Summer'= 3, 'Autumn'= 4)
clean_bike$seasons <- as.factor(clean_bike$seasons)
class(clean_bike$seasons)

clean_bike$day <- weekdays(as.Date(clean_bike$date))
clean_bike$day <- as.factor(clean_bike$day)

```

By using `table` function it is possible now to count all the values. It is clear now that 'functioning day' has only 1 value. It means that bikes are rented in Seoul any time of the year and any time of the day. So we can remove this variable from the data frame 'clean_bike'.

```{r tables for analysis}

table(clean_bike$functioning_day)

pie(table(clean_bike$holiday), labels=c("No","Yes"))
table(clean_bike$holiday)

pie(table(clean_bike$seasons), labels=c("Winter","Spring","Summer","Autumn"))
table(clean_bike$seasons)

pie(table(clean_bike$day),labels=c("Monday","Tuesday","Wednesday","Thursday","Friday","Saturday","Sunday"))
table(clean_bike$day)

clean_bike <- clean_bike %>% select(-functioning_day)
```

Now I can calculate correlation and look for variables that correlate the most with `rented_bike_count`. In other words, we are looking for the most paramount factors for bike rent. Also 'dew_point_temperature' is removed because it highly correlates with temperature and for predictive analysis it won't be useful. One 'temperature' variable is enough for analysis. It can be seen that 

```{r correlation , out.width='100%', fig.align = 'left', fig.cap= "Figure 1.1 This is Caption"}
corrplot(cor(clean_bike[c(2:11)]),type = "upper",col=COL2("RdBu",100))
clean_bike <- clean_bike %>% select(-dew_point_temperature)
```

With the help of `grouping`, `average_bike_rent_coount` and 'average_temperature` is calculated per month and year since there is one month of 2017. The most bikes were rented during June `n=1237` and September `n=1079` and temperatures 'T1=23.1' and `T2=21.7` accordingly. 

```{r tables, message = FALSE }


rent_by_year <- clean_bike %>%
  mutate(year= year(clean_bike$date), month=month(clean_bike$date)) %>%
  group_by(year, month) %>%
  summarise(total = sum(rented_bike_count), average_bike_rent_count = mean(rented_bike_count), average_temperature = mean(temperature))
kable(rent_by_year)
```
Furthermore, seasonal bike rental analysis is carried out by using `date` and `seasons` variables in addition. It is evident that bike rent is highly dependent on the temperature. The lowest rent numbers are generated during the winter and highest in the summer. Also other was made with highlighted `holiday` points. There is no visual difference of bike rent during the holidays, so we can ignore it, during the analysis.


# Seasonal

```{r , out.width='100%', fig.align = 'left', fig.cap= "Figure 1.1 This is Caption", warning= FALSE}

ggplot(clean_bike, aes( x=date, y =rented_bike_count, color= temperature)) + geom_point() +
  facet_wrap(~seasons, scales = "free_x") +
ylab('Count of Rented Bikes')  +
xlab('Date') +
ggtitle('Seasonal Bike Rent Dependency in Seoul')  + 
 scale_fill_viridis(direction = -1)+
  scale_colour_viridis_c(option = "plasma")


highlight <- clean_bike %>% filter(holiday==1)

  ggplot(clean_bike, aes( x=date, y =rented_bike_count, color= temperature)) + geom_point(alpha=0.3) +
 geom_point(data= highlight , aes(x=date, y =rented_bike_count), color ='black', size=4) +
ylab('Count of Rented Bikes')  +
xlab('Date') +
ggtitle('Seasonal Bike Rent Dependency in Seoul with highlighted holidays')  + 
    scale_fill_viridis(direction = -1)+
  scale_colour_viridis_c(option = "plasma")

```


# Hour

After Using `table` function on `hour` variable it is clear that each hour has the same amount of records.
The bar plots shows hourly rented bike dependency. The least amount bikes is rented 4 in the morning. There is a spiking in a graph during 8 in the morning. It is probably due to people going to work. The highest number of bikes rented is at the 18th hour. Around that time people finish working in Korea, so they are going home after work.

```{r hour-a}
table(clean_bike$hour)

ggplot(clean_bike, aes(x= hour, y = rented_bike_count)) + geom_col() +  scale_x_continuous(breaks=seq(0,23,1)) + ggtitle("Bike rent count dependancy on hour")
```

Let's look for an outliers for each hour with the help of `boxplot`, `dplyr` package and `indtify_outliers` function, which looks for outliers with the help of first and third `quantiles` and `IQR`. 2 outlier points ar visible from the boxplot. After printing all the records with these outliers, we can see that there is nothing special about them, so they are removed using `anti-join`

```{r hour-b}

ggplot(clean_bike, aes(x= hour, y = rented_bike_count)) + geom_boxplot(aes(group = hour)) +  scale_x_continuous(breaks=seq(0,23,1)) +
  coord_flip() + ggtitle("Bike rent count dependancy on hour")

outliers_hour <- clean_bike %>%
  group_by(hour) %>%
  identify_outliers(rented_bike_count)
outliers_hour

clean_bike <- clean_bike %>% anti_join(outliers_hour, by = c( "hour", "rented_bike_count"))

```

Very obvious pattern is observed when analyzing hourly bike rent by the day. During weekends  graphs are much smoother(ne peaks, like on weekdays) observed due to the fact that majority of people don't work on Saturday and Sunday.
```{r hour-c}
ggplot(clean_bike, aes(x= hour, y = rented_bike_count, fill = day)) + geom_col() +
scale_x_continuous(breaks=seq(0,23,2))+ facet_wrap(~day) + theme(legend.position='none' ) + ggtitle("Daily bike rent count dependancy on hour")
```







Moving on, there is obviuos correlation between temperature and bike rent
# Temperature
```{r temp}
hist(clean_bike$temperature  ,xlab= "Temperature", main= "Temperature distribution", xaxp = c(-20,40,24) )

summary(clean_bike$temperature)

ggplot(clean_bike, aes(temperature, rented_bike_count)) + geom_point() + ylim(0,4000)
ggplot(clean_bike, aes(hour, temperature)) + geom_boxplot() 


```







## Modeling analysis

Data is split into training and test data sets with 70/30 % ratio

```{r data splitting}
split = sort(sample(nrow(clean_bike), nrow(clean_bike)*.7))
train<-clean_bike[split,]
test<-clean_bike[-split,]
dim(train)
dim(test)
glimpse(train)

```

# Mars

MARS

```{r mars, eval = FALSE}

marsm <- earth(rented_bike_count ~ ., data = train, degree = 1)


marsm
summary(marsm)
plot(marsm, which = 1)
axis(1, at = 1:20)

knitr::knit_exit()


```

```{r mars2, eval = FALSE}
hyper_grid <- expand.grid(
  degree = 1:3, 
  nprune = seq(2, 30, length.out = 10) %>% floor()
  )

tuned_mars <- train(
  x = subset(train, select = -rented_bike_count),
  y = train$rented_bike_count,
  method = "earth",
  metric = "RMSE",
  trControl = trainControl(method = "cv", number = 10),
  tuneGrid = hyper_grid
)
tuned_mars$bestTune

ggplot(tuned_mars)+
  scale_x_continuous(breaks = seq(0, 30, by = 5))


p1 <- vip(tuned_mars, num_features = 20, bar = FALSE, value = "gcv") + ggtitle("GCV")
p2 <- vip(tuned_mars, num_features = 20, bar = FALSE, value = "rss") + ggtitle("RSS")

gridExtra::grid.arrange(p1, p2, ncol = 2)


```


# decisions tree


```{r decision tree}
treem <- rpart(rented_bike_count ~ ., 
             method = "anova", data = train)
treem

plot(treem, uniform = TRUE,
          main = "... 
                 Tree using Regression")
text(treem, use.n = TRUE, cex = .7)

# Step 1 - create the evaluation metrics function


eval_results <- function(true, predicted, df) {

  SSE <- sum((predicted - true)^2)

  SST <- sum((true - mean(true))^2)

  R_square <- 1 - SSE / SST

  RMSE = sqrt(SSE/nrow(df))

  

# Model performance metrics

  data.frame(

    RMSE = RMSE,

    Rsquare = R_square

  )

  

}


# Step 2 - predicting and evaluating the model on train data


predictions_train_cart = predict(treem, data = train)

eval_results(train$rented_bike_count, predictions_train_cart, train)


# Step 3 - predicting and evaluating the model on test data


predictions_test_cart = predict(treem, newdata = test)

eval_results(test$rented_bike_count, predictions_test_cart, test)

```
# gam model
```{r GAMs}
GAMsm <- gam(rented_bike_count ~ .,data = train)
summary(GAMsm)



```


# GBM
```{r GBM}
model_gbm <- gbm(formula = rented_bike_count ~.,
                data = train[,c(-1,-13)],
                distribution = "gaussian",
               cv.folds = 5,
               interaction.depth = 2,
                shrinkage = .01,
                n.minobsinnode = 10,
                n.trees = 5000,
               n.cores = NULL, # will use all cores by defaul
               verbose = FALSE)
 

print(model_gbm)

# model performance
perf_gbm1 = gbm.perf(model_gbm, method = "cv")
print(perf_gbm1)


bike_prediction_1 <- stats::predict(
                           # the model from above
                          object = model_gbm, 
                          # the testing data
                          newdata = test,
                          # this is the number we calculated above
                          n.trees = perf_gbm1)

rmse_fit1 <- Metrics::rmse(actual = test$rented_bike_count, 
                           predicted = bike_prediction_1)


print(rmse_fit1)

min_MSE <- which.min(model_gbm$cv.error)

# get MSE and compute RMSE
sqrt(model_gbm$cv.error[min_MSE])
## [1] 23112.1

# plot loss function as a result of n trees added to the ensemble
gbm.perf( model_gbm, method = "cv")


```


# XGboost 
```{r XGBoost, eval = FALSE}


X_train = data.matrix(train[,-2])                  # independent variables for train
y_train = train[,2]                                # dependent variables for train
  
X_test = data.matrix(test[,-2])                    # independent variables for test
y_test = test[,2]                                   # dependent variables for test

# convert the train and test data into xgboost matrix type.
xgboost_train = xgb.DMatrix(data=X_train, label=y_train)
xgboost_test = xgb.DMatrix(data=X_test, label=y_test)


xgm <- xgboost(data = xgboost_train,                    # the data   
                 max.depth=3,                # max depth 
               scale_pos_weight = 1, 
               early_stopping_rounds = 3,
               mode="regression",
                 nrounds=5000)                              # max number of boosting iterations

summary(xgm)
xgm

#pred_test <- predict(xgm, xgboost_test)

#pred_test


```

# Time-series forecast



``` {r  }


```
